üé¨ Project: Simple Movie Data Pipeline
üìÅ Final Folder Structure
movie-data-pipeline/
‚îú‚îÄ‚îÄ etl.py
‚îú‚îÄ‚îÄ schema.sql
‚îú‚îÄ‚îÄ queries.sql
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md

üß± 1. schema.sql

This defines your database structure.

-- schema.sql
DROP TABLE IF EXISTS movies;
DROP TABLE IF EXISTS ratings;

CREATE TABLE movies (
    movie_id INTEGER PRIMARY KEY,
    title TEXT,
    genres TEXT,
    director TEXT,
    plot TEXT,
    box_office TEXT,
    year INTEGER
);

CREATE TABLE ratings (
    rating_id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER,
    movie_id INTEGER,
    rating REAL,
    timestamp TEXT,
    FOREIGN KEY (movie_id) REFERENCES movies(movie_id)
);


‚úÖ Design Rationale

movies holds metadata about each film.

ratings links user ratings to specific movies.

year extracted from OMDb or title (if not available).

Foreign key ensures referential integrity.

# üé¨ etl.py 
import pandas as pd
import requests
import time
from sqlalchemy import create_engine, text

# ================= CONFIG =================
MOVIES_CSV = "movies.csv"
RATINGS_CSV = "ratings.csv"
OMDB_API_KEY = "YOUR_API_KEY"   # üîë Replace this with your actual OMDb API key
DB_NAME = "movies.db"
OMDB_URL = "http://www.omdbapi.com/"
# ==========================================


def get_omdb_data(title):
    """Fetch movie details from OMDb API."""
    params = {"t": title, "apikey": OMDB_API_KEY}
    try:
        response = requests.get(OMDB_URL, params=params, timeout=5)
        data = response.json()
        if data.get("Response") == "True":
            return {
                "director": data.get("Director"),
                "plot": data.get("Plot"),
                "box_office": data.get("BoxOffice"),
                "year": data.get("Year")
            }
        else:
            return {"director": None, "plot": None, "box_office": None, "year": None}
    except Exception as e:
        print(f"‚ö† API error for '{title}': {e}")
        return {"director": None, "plot": None, "box_office": None, "year": None}


def extract_data():
    """Extract movie and rating data from CSV files."""
    print(" Extracting data from CSV files...")
    movies_df = pd.read_csv(MOVIES_CSV)
    ratings_df = pd.read_csv(RATINGS_CSV)
    print("Data extracted successfully.")
    return movies_df, ratings_df


def transform_data(movies_df, ratings_df):
    """Transform and enrich data using OMDb API."""
    print(" Enriching movie data with OMDb API...")
    enriched_data = []

    for _, row in movies_df.iterrows():
        title = row["title"]
        omdb_info = get_omdb_data(title)
        enriched_row = {
            "title": title,
            "genres": row.get("genres", "Unknown"),
            "director": omdb_info["director"] or "Unknown",
            "plot": omdb_info["plot"] or "Unknown",
            "box_office": omdb_info["box_office"] or "Unknown",
            "year": pd.to_numeric(omdb_info["year"], errors="coerce")
        }
        enriched_data.append(enriched_row)
        time.sleep(0.2)  # Prevent hitting OMDb rate limits

    movies_clean = pd.DataFrame(enriched_data)
    ratings_df = ratings_df.rename(columns={"movieId": "movie_id"})

    print("Data transformed successfully.")
    return movies_clean, ratings_df


def load_data(movies_df, ratings_df):
    """Load data into SQLite database."""
    print(" Loading data into SQLite database...")
    engine = create_engine(f"sqlite:///{DB_NAME}")

    with engine.begin() as conn:
        # Read the schema
        with open("schema.sql", "r") as f:
            schema_sql = f.read()

        # Execute SQL statements one at a time
        for statement in schema_sql.strip().split(";"):
            if statement.strip():
                conn.exec_driver_sql(statement)

        # Load data
        movies_df.to_sql("movies", conn, if_exists="append", index=False)
        ratings_df.to_sql("ratings", conn, if_exists="append", index=False)

    print(" Data successfully loaded into database.")


def main():
    print(" Starting ETL pipeline...")
    movies_df, ratings_df = extract_data()
    movies_clean, ratings_clean = transform_data(movies_df, ratings_df)
    load_data(movies_clean, ratings_clean)
    print(" ETL process completed successfully!")


if __name__ == "__main__":
    main()


‚úÖ Notes

Extract: Reads CSVs using pandas.

Transform: Calls OMDb API to enrich each movie.

Load: Uses SQLAlchemy to insert into SQLite DB.

Idempotency: Drops & recreates schema before load (can be modified).

üìä 3. queries.sql
-- queries.sql

-- 1. Movie with the highest average rating
SELECT m.title, AVG(r.rating) AS avg_rating
FROM ratings r
JOIN movies m ON r.movie_id = m.movie_id
GROUP BY m.title
ORDER BY avg_rating DESC
LIMIT 1;

-- 2. Top 5 genres by average rating
SELECT 
    TRIM(genre) AS genre, 
    AVG(r.rating) AS avg_rating
FROM movies m
JOIN ratings r ON m.movie_id = r.movie_id,
     LATERAL split(m.genres, '|') AS genre
GROUP BY genre
ORDER BY avg_rating DESC
LIMIT 5;

-- 3. Director with most movies
SELECT director, COUNT(*) AS movie_count
FROM movies
GROUP BY director
ORDER BY movie_count DESC
LIMIT 1;

-- 4. Average rating by year
SELECT m.year, AVG(r.rating) AS avg_rating
FROM ratings r
JOIN movies m ON m.movie_id = r.movie_id
GROUP BY m.year
ORDER BY m.year;


(SQLite doesn‚Äôt support split() natively ‚Äî for testing, you can flatten genres in Python before loading or use a custom UDF.)

üì¶ 4. requirements.txt
pandas
requests
sqlalchemy


(Optional: add sqlite3 if packaging for external envs, though it‚Äôs built-in.)

üìò 5. README.md
# üé¨ Simple Movie Data Pipeline

## Overview
This project builds a small ETL (Extract, Transform, Load) pipeline for movie data using **Python, SQLite, and the OMDb API**.

The pipeline:
- Reads local movie and rating CSVs (from [MovieLens](https://grouplens.org/datasets/movielens/latest/))
- Enriches movie data with additional details (director, plot, box office, year) from OMDb
- Cleans and loads everything into a relational database
- Runs SQL queries to answer key analytical questions

---

## üß∞ Tech Stack
- Python 3.x
- SQLite
- Pandas
- SQLAlchemy
- Requests (for API calls)

---

## ‚öôÔ∏è Setup Instructions

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/movie-data-pipeline.git
   cd movie-data-pipeline


Install dependencies:

pip install -r requirements.txt


Download the MovieLens dataset:

From: https://grouplens.org/datasets/movielens/latest/

Place movies.csv and ratings.csv in the same folder as etl.py.

Get a free OMDb API key:

Visit http://www.omdbapi.com/

Replace YOUR_API_KEY in etl.py with your key.

Run the pipeline:

python etl.py


Explore the database:

sqlite3 movies.db
.read queries.sql

üìà Analytical Questions

Movie with the highest average rating

Top 5 genres by average rating

Director with most movies

Average rating per year

üí° Design Choices & Assumptions

SQLite chosen for simplicity; easily replaced with PostgreSQL or MySQL.

Idempotent load process by dropping/recreating schema.

API requests are rate-limited with time.sleep(0.2).

Movies not found in OMDb are still included with null metadata.

üöß Future Improvements

Add caching for API responses.

Normalize genres into a separate table.

Implement incremental ETL instead of full reload.

Add Airflow or Prefect orchestration for automation.

üë®‚Äçüíª Author

Prepared for tsworks Data Engineering Assignment.






