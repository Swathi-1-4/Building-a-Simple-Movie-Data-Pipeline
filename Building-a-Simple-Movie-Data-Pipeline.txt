üé¨ Project: Simple Movie Data Pipeline
üìÅ Final Folder Structure
movie-data-pipeline/
‚îú‚îÄ‚îÄ etl.py
‚îú‚îÄ‚îÄ schema.sql
‚îú‚îÄ‚îÄ queries.sql
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md

üß± 1. schema.sql

This defines your database structure.

-- schema.sql
DROP TABLE IF EXISTS movies;
DROP TABLE IF EXISTS ratings;

CREATE TABLE movies (
    movie_id INTEGER PRIMARY KEY,
    title TEXT,
    genres TEXT,
    director TEXT,
    plot TEXT,
    box_office TEXT,
    year INTEGER
);

CREATE TABLE ratings (
    rating_id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER,
    movie_id INTEGER,
    rating REAL,
    timestamp TEXT,
    FOREIGN KEY (movie_id) REFERENCES movies(movie_id)
);


‚úÖ Design Rationale

movies holds metadata about each film.

ratings links user ratings to specific movies.

year extracted from OMDb or title (if not available).

Foreign key ensures referential integrity.

üß† 2. etl.py

This is the main ETL pipeline script.

# etl.py
import pandas as pd
import requests
import sqlite3
import time
from sqlalchemy import create_engine, text

# ========== CONFIG ==========
MOVIES_CSV = "movies.csv"
RATINGS_CSV = "ratings.csv"
OMDB_API_KEY = "YOUR_API_KEY"  # Replace with your OMDb API key
DB_NAME = "movies.db"
OMDB_URL = "http://www.omdbapi.com/"

# ============================

def get_omdb_data(title):
    """Fetch movie details from OMDb API."""
    params = {"t": title, "apikey": OMDB_API_KEY}
    try:
        response = requests.get(OMDB_URL, params=params, timeout=5)
        data = response.json()
        if data.get("Response") == "True":
            return {
                "Director": data.get("Director", None),
                "Plot": data.get("Plot", None),
                "BoxOffice": data.get("BoxOffice", None),
                "Year": data.get("Year", None)
            }
        else:
            return {"Director": None, "Plot": None, "BoxOffice": None, "Year": None}
    except Exception as e:
        print(f"API error for {title}: {e}")
        return {"Director": None, "Plot": None, "BoxOffice": None, "Year": None}

def extract_data():
    """Read local CSV files."""
    movies_df = pd.read_csv(MOVIES_CSV)
    ratings_df = pd.read_csv(RATINGS_CSV)
    return movies_df, ratings_df

def transform_data(movies_df, ratings_df):
    """Clean, enrich, and merge data."""
    print("Fetching additional movie data from OMDb API...")
    omdb_details = []
    for _, row in movies_df.iterrows():
        details = get_omdb_data(row['title'])
        omdb_details.append(details)
        time.sleep(0.2)  # Avoid rate-limiting

    omdb_df = pd.DataFrame(omdb_details)
    movies_enriched = pd.concat([movies_df, omdb_df], axis=1)

    # Clean up columns
    movies_enriched['Year'] = pd.to_numeric(movies_enriched['Year'], errors='coerce')
    movies_enriched.fillna("Unknown", inplace=True)

    return movies_enriched, ratings_df

def load_data(movies_df, ratings_df):
    """Load data into SQLite database."""
    engine = create_engine(f"sqlite:///{DB_NAME}")
    with engine.connect() as conn:
        # Create tables if not exist
        with open("schema.sql", "r") as f:
            conn.execute(text(f.read()))

        # Load data
        movies_df.to_sql("movies", conn, if_exists="append", index=False)
        ratings_df.to_sql("ratings", conn, if_exists="append", index=False)

        print("Data successfully loaded into database.")

def main():
    print("Starting ETL pipeline...")
    movies_df, ratings_df = extract_data()
    movies_clean, ratings_clean = transform_data(movies_df, ratings_df)
    load_data(movies_clean, ratings_clean)
    print("ETL process completed successfully!")

if __name__ == "__main__":
    main()


‚úÖ Notes

Extract: Reads CSVs using pandas.

Transform: Calls OMDb API to enrich each movie.

Load: Uses SQLAlchemy to insert into SQLite DB.

Idempotency: Drops & recreates schema before load (can be modified).

üìä 3. queries.sql
-- queries.sql

-- 1. Movie with the highest average rating
SELECT m.title, AVG(r.rating) AS avg_rating
FROM ratings r
JOIN movies m ON r.movie_id = m.movie_id
GROUP BY m.title
ORDER BY avg_rating DESC
LIMIT 1;

-- 2. Top 5 genres by average rating
SELECT 
    TRIM(genre) AS genre, 
    AVG(r.rating) AS avg_rating
FROM movies m
JOIN ratings r ON m.movie_id = r.movie_id,
     LATERAL split(m.genres, '|') AS genre
GROUP BY genre
ORDER BY avg_rating DESC
LIMIT 5;

-- 3. Director with most movies
SELECT director, COUNT(*) AS movie_count
FROM movies
GROUP BY director
ORDER BY movie_count DESC
LIMIT 1;

-- 4. Average rating by year
SELECT m.year, AVG(r.rating) AS avg_rating
FROM ratings r
JOIN movies m ON m.movie_id = r.movie_id
GROUP BY m.year
ORDER BY m.year;


(SQLite doesn‚Äôt support split() natively ‚Äî for testing, you can flatten genres in Python before loading or use a custom UDF.)

üì¶ 4. requirements.txt
pandas
requests
sqlalchemy


(Optional: add sqlite3 if packaging for external envs, though it‚Äôs built-in.)

üìò 5. README.md
# üé¨ Simple Movie Data Pipeline

## Overview
This project builds a small ETL (Extract, Transform, Load) pipeline for movie data using **Python, SQLite, and the OMDb API**.

The pipeline:
- Reads local movie and rating CSVs (from [MovieLens](https://grouplens.org/datasets/movielens/latest/))
- Enriches movie data with additional details (director, plot, box office, year) from OMDb
- Cleans and loads everything into a relational database
- Runs SQL queries to answer key analytical questions

---

## üß∞ Tech Stack
- Python 3.x
- SQLite
- Pandas
- SQLAlchemy
- Requests (for API calls)

---

## ‚öôÔ∏è Setup Instructions

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/movie-data-pipeline.git
   cd movie-data-pipeline


Install dependencies:

pip install -r requirements.txt


Download the MovieLens dataset:

From: https://grouplens.org/datasets/movielens/latest/

Place movies.csv and ratings.csv in the same folder as etl.py.

Get a free OMDb API key:

Visit http://www.omdbapi.com/

Replace YOUR_API_KEY in etl.py with your key.

Run the pipeline:

python etl.py


Explore the database:

sqlite3 movies.db
.read queries.sql

üìà Analytical Questions

Movie with the highest average rating

Top 5 genres by average rating

Director with most movies

Average rating per year

üí° Design Choices & Assumptions

SQLite chosen for simplicity; easily replaced with PostgreSQL or MySQL.

Idempotent load process by dropping/recreating schema.

API requests are rate-limited with time.sleep(0.2).

Movies not found in OMDb are still included with null metadata.

üöß Future Improvements

Add caching for API responses.

Normalize genres into a separate table.

Implement incremental ETL instead of full reload.

Add Airflow or Prefect orchestration for automation.

üë®‚Äçüíª Author

Prepared for tsworks Data Engineering Assignment.


---

Would you like me to **bundle all these files** into a ready-to-download `.zip` project (so you can run or upload it to GitHub right away)?
